{
 "cells": [
  {
   "cell_type": "code",
   "source": "import json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport re\nfrom pathlib import Path\n\n# Load the field recall results and excluded fields\nwith open('../field_recall_results.json', 'r') as f:\n    data = json.load(f)\n\nwith open('../info.json', 'r') as f:\n    info_data = json.load(f)\n\nfield_recall_data = data['field_recall_by_model']\nexcluded_fields = info_data.get('excluded_fields', [])\n\n# Strip all punctuation including underscores for comparison\ndef strip_punctuation(text):\n    return re.sub(r'[^a-zA-Z0-9]', '', text)\n\nexcluded_fields_clean = [strip_punctuation(field) for field in excluded_fields]\n\nprint(f\"Loaded data for {len(field_recall_data)} models\")\nprint(f\"Models: {list(field_recall_data.keys())}\")\nprint(f\"Excluded {len(excluded_fields)} fields\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create heatmap data from field recall results\nheatmap_data = []\n\nfor model, fields in field_recall_data.items():\n    for field, score in fields.items():\n        # Strip punctuation from field name for excluded field comparison\n        field_clean = strip_punctuation(field)\n        if field_clean not in excluded_fields_clean:\n            heatmap_data.append({\n                'model': model,\n                'field': field,\n                'score': score\n            })\n\n# Convert to DataFrame and pivot for heatmap\ndf = pd.DataFrame(heatmap_data)\nheatmap_data_pivot = df.pivot(index='field', columns='model', values='score')\n\n# Sort fields by average score (highest to lowest)\nfield_avg_scores = heatmap_data_pivot.mean(axis=1)\nheatmap_data_sorted = heatmap_data_pivot.loc[field_avg_scores.sort_values(ascending=False).index]\n\nprint(f\"Overall heatmap shape: {heatmap_data_sorted.shape}\")\nprint(f\"Fields included: {heatmap_data_sorted.shape[0]}\")\nprint(f\"Models included: {heatmap_data_sorted.shape[1]}\")\n\n# Create the overall heatmap visualization\nplt.style.use('default')  \nfig, ax = plt.subplots(figsize=(14, 16))\n\n# Create sophisticated orange-to-green colormap\ncolors = ['#CC4125', '#E55100', '#FF6F00', '#FF8F00', '#FFB300', '#FFCC02', \n          '#C5E1A5', '#A5D6A7', '#81C784', '#66BB6A', '#4CAF50', '#388E3C']\nn_bins = 256\ncmap = sns.blend_palette(colors, n_colors=n_bins, as_cmap=True)\n\n# Create heatmap\nax = sns.heatmap(heatmap_data_sorted, \n                 annot=True, \n                 fmt='.2f', \n                 cmap=cmap,\n                 vmin=0, \n                 vmax=1,\n                 cbar_kws={\n                     'label': 'Field Recall Score (All PMIDs)',\n                     'shrink': 0.8,\n                     'aspect': 30,\n                     'pad': 0.02\n                 },\n                 annot_kws={'size': 9, 'weight': 'bold', 'color': 'white'},\n                 linewidths=0.5,\n                 linecolor='white',\n                 square=False,\n                 xticklabels=True,\n                 yticklabels=True,\n                 ax=ax)\n\n# Style the plot\nplt.title('Field Recall Scores by Model (All PMIDs)\\n(Ordered by Average Score: High to Low)\\n(Orange = Low Score, Green = High Score)', \n          fontsize=18, fontweight='bold', pad=30, color='#2C3E50')\nplt.xlabel('Models', fontsize=14, fontweight='bold', color='#34495E')\nplt.ylabel('Fields', fontsize=14, fontweight='bold', color='#34495E')\n\n# Style the tick labels\nplt.xticks(rotation=45, ha='right', fontsize=11, color='#2C3E50')\nplt.yticks(rotation=0, fontsize=10, color='#2C3E50')\n\n# Add model labels at the top\nax2 = ax.twiny()\nax2.set_xlim(ax.get_xlim())\nax2.set_xticks(ax.get_xticks())\nax2.set_xticklabels(heatmap_data_sorted.columns, rotation=45, ha='left', \n                    fontsize=11, color='#2C3E50', weight='bold')\nax2.set_xlabel('Models', fontsize=14, fontweight='bold', color='#34495E', pad=15)\n\n# Remove spines for cleaner look\nfor spine in ax.spines.values():\n    spine.set_visible(False)\nfor spine in ax2.spines.values():\n    spine.set_visible(False)\n\n# Add subtle background\nfig.patch.set_facecolor('#FAFAFA')\nax.set_facecolor('#FAFAFA')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Overall summary statistics\nprint(\"=== FIELD RECALL SUMMARY (ALL PMIDs) ===\\n\")\n\n# Overall model performance\nmodel_avg_scores = heatmap_data_sorted.mean(axis=0).sort_values(ascending=False)\nprint(\"Average scores by model:\")\nfor model, score in model_avg_scores.items():\n    print(f\"  {model}: {score:.3f}\")\n\nprint(f\"\\nBest performing model: {model_avg_scores.index[0]} ({model_avg_scores.iloc[0]:.3f})\")\nprint(f\"Worst performing model: {model_avg_scores.index[-1]} ({model_avg_scores.iloc[-1]:.3f})\")\n\n# Field difficulty analysis (already sorted in heatmap)\nfield_avg_scores = heatmap_data_sorted.mean(axis=1)\nprint(f\"\\nEasiest fields (top 5):\")\nfor field, score in field_avg_scores.head().items():\n    print(f\"  {field}: {score:.3f}\")\n\nprint(f\"\\nHardest fields (bottom 5):\")\nfor field, score in field_avg_scores.tail().items():\n    print(f\"  {field}: {score:.3f}\")\n\n# Zero score fields\nzero_scores = heatmap_data_sorted[heatmap_data_sorted == 0].stack()\nprint(f\"\\nFields with zero scores: {len(zero_scores)} instances\")\nif len(zero_scores) > 0:\n    print(\"Zero score fields:\")\n    for (field, model), _ in zero_scores.items():\n        print(f\"  {model}: {field}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load SCR PMIDs from experiment.json and create filtered visualization\nwith open('../../../experiment.json', 'r') as f:\n    experiment_data = json.load(f)\n\nscr_pmids = set(experiment_data.get('scr_pmids', []))\nprint(f\"Found {len(scr_pmids)} SCR PMIDs to filter by\")\n\n# Load field results to get raw comparison data\nfield_results_dir = Path('../field_results')\nfield_files = list(field_results_dir.glob('*.json'))\n\n# Filter data to only include SCR PMIDs\nscr_filtered_data = []\ntotal_comparisons = 0\nscr_comparisons = 0\n\nfor field_file in field_files:\n    if field_file.stem in ['README', 'visualisation']:\n        continue\n        \n    try:\n        with open(field_file, 'r') as f:\n            field_data = json.load(f)\n    except:\n        print(f\"Could not read {field_file}\")\n        continue\n    \n    field_path = field_data.get('field_path', field_file.stem.replace('_', '.'))\n    \n    # Filter comparisons to only SCR PMIDs\n    for comparison in field_data.get('comparisons', []):\n        total_comparisons += 1\n        pmid = comparison.get('pmid', '')\n        \n        if pmid in scr_pmids:\n            scr_comparisons += 1\n            # Strip punctuation from field name for excluded field comparison\n            field_clean = strip_punctuation(field_path)\n            if field_clean not in excluded_fields_clean:\n                scr_filtered_data.append({\n                    'model': comparison['model_name'],\n                    'field': field_path,\n                    'pmid': pmid,\n                    'ground_truth': comparison.get('ground_truth', ''),\n                    'model_output': comparison.get('model_output', ''),\n                    'match': comparison.get('ground_truth') == comparison.get('model_output')\n                })\n\nprint(f\"Total comparisons: {total_comparisons}\")\nprint(f\"SCR PMID comparisons: {scr_comparisons}\")\nprint(f\"SCR comparisons after filtering excluded fields: {len(scr_filtered_data)}\")\n\n# Convert to DataFrame\nscr_df = pd.DataFrame(scr_filtered_data)\n\nif len(scr_df) > 0:\n    # Calculate recall scores for SCR PMIDs only\n    scr_recall_scores = []\n    for model in scr_df['model'].unique():\n        for field in scr_df['field'].unique():\n            model_field_data = scr_df[(scr_df['model'] == model) & (scr_df['field'] == field)]\n            if len(model_field_data) > 0:\n                recall = model_field_data['match'].mean()\n                scr_recall_scores.append({\n                    'model': model,\n                    'field': field,\n                    'score': recall\n                })\n\n    scr_recall_df = pd.DataFrame(scr_recall_scores)\n    scr_heatmap_data = scr_recall_df.pivot(index='field', columns='model', values='score')\n    \n    # Sort by average score\n    scr_field_avg_scores = scr_heatmap_data.mean(axis=1)\n    scr_heatmap_data_sorted = scr_heatmap_data.loc[scr_field_avg_scores.sort_values(ascending=False).index]\n    \n    print(f\"SCR-filtered heatmap shape: {scr_heatmap_data_sorted.shape}\")\nelse:\n    print(\"No SCR PMID data found after filtering!\")\n    scr_df = pd.DataFrame()\n    scr_heatmap_data_sorted = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create SCR-filtered heatmap visualization\nif len(scr_df) > 0 and scr_heatmap_data_sorted is not None:\n    plt.style.use('default')  \n    fig, ax = plt.subplots(figsize=(14, 16))\n\n    # Use same sophisticated orange-to-green colormap\n    colors = ['#CC4125', '#E55100', '#FF6F00', '#FF8F00', '#FFB300', '#FFCC02', \n              '#C5E1A5', '#A5D6A7', '#81C784', '#66BB6A', '#4CAF50', '#388E3C']\n    n_bins = 256\n    cmap = sns.blend_palette(colors, n_colors=n_bins, as_cmap=True)\n\n    # Create heatmap with SCR-filtered data\n    ax = sns.heatmap(scr_heatmap_data_sorted, \n                     annot=True, \n                     fmt='.2f', \n                     cmap=cmap,\n                     vmin=0, \n                     vmax=1,\n                     cbar_kws={\n                         'label': 'Field Recall Score (SCR PMIDs Only)',\n                         'shrink': 0.8,\n                         'aspect': 30,\n                         'pad': 0.02\n                     },\n                     annot_kws={'size': 9, 'weight': 'bold', 'color': 'white'},\n                     linewidths=0.5,\n                     linecolor='white',\n                     square=False,\n                     xticklabels=True,\n                     yticklabels=True,\n                     ax=ax)\n\n    # Style the plot\n    plt.title(f'Field Recall Scores by Model (SCR PMIDs Only)\\n({len(scr_pmids)} SCR Papers, {len(scr_df)} Comparisons)\\n(Orange = Low Score, Green = High Score)', \n              fontsize=18, fontweight='bold', pad=30, color='#2C3E50')\n    plt.xlabel('Models', fontsize=14, fontweight='bold', color='#34495E')\n    plt.ylabel('Fields', fontsize=14, fontweight='bold', color='#34495E')\n\n    # Style the tick labels\n    plt.xticks(rotation=45, ha='right', fontsize=11, color='#2C3E50')\n    plt.yticks(rotation=0, fontsize=10, color='#2C3E50')\n\n    # Add model labels at the top\n    ax2 = ax.twiny()\n    ax2.set_xlim(ax.get_xlim())\n    ax2.set_xticks(ax.get_xticks())\n    ax2.set_xticklabels(scr_heatmap_data_sorted.columns, rotation=45, ha='left', \n                        fontsize=11, color='#2C3E50', weight='bold')\n    ax2.set_xlabel('Models', fontsize=14, fontweight='bold', color='#34495E', pad=15)\n\n    # Remove spines for cleaner look\n    for spine in ax.spines.values():\n        spine.set_visible(False)\n    for spine in ax2.spines.values():\n        spine.set_visible(False)\n\n    # Add subtle background\n    fig.patch.set_facecolor('#FAFAFA')\n    ax.set_facecolor('#FAFAFA')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Cannot create SCR-filtered visualization - no data available\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SCR-filtered summary statistics\nif len(scr_df) > 0 and scr_heatmap_data_sorted is not None:\n    print(\"=== SCR-FILTERED FIELD RECALL SUMMARY ===\\n\")\n\n    # Overall model performance on SCR PMIDs\n    scr_model_avg_scores = scr_heatmap_data_sorted.mean(axis=0).sort_values(ascending=False)\n    print(\"Average scores by model (SCR PMIDs only):\")\n    for model, score in scr_model_avg_scores.items():\n        print(f\"  {model}: {score:.3f}\")\n\n    print(f\"\\nBest performing model on SCR PMIDs: {scr_model_avg_scores.index[0]} ({scr_model_avg_scores.iloc[0]:.3f})\")\n    print(f\"Worst performing model on SCR PMIDs: {scr_model_avg_scores.index[-1]} ({scr_model_avg_scores.iloc[-1]:.3f})\")\n\n    # Field difficulty analysis for SCR PMIDs\n    scr_field_avg_scores = scr_heatmap_data_sorted.mean(axis=1)\n    print(f\"\\nEasiest fields on SCR PMIDs (top 5):\")\n    for field, score in scr_field_avg_scores.head().items():\n        print(f\"  {field}: {score:.3f}\")\n\n    print(f\"\\nHardest fields on SCR PMIDs (bottom 5):\")\n    for field, score in scr_field_avg_scores.tail().items():\n        print(f\"  {field}: {score:.3f}\")\n\n    # Compare with overall performance\n    print(f\"\\n=== COMPARISON: ALL vs SCR PMIDs ===\")\n    print(\"Model performance comparison (All PMIDs vs SCR PMIDs only):\")\n    overall_model_scores = heatmap_data_sorted.mean(axis=0)\n    \n    for model in overall_model_scores.index:\n        overall_score = overall_model_scores[model]\n        scr_score = scr_model_avg_scores.get(model, 0.0)\n        difference = scr_score - overall_score\n        direction = \"↑\" if difference > 0 else \"↓\" if difference < 0 else \"=\"\n        print(f\"  {model}: {overall_score:.3f} → {scr_score:.3f} ({difference:+.3f} {direction})\")\n\n    # Zero score fields on SCR PMIDs\n    scr_zero_scores = scr_heatmap_data_sorted[scr_heatmap_data_sorted == 0].stack()\n    print(f\"\\nFields with zero scores on SCR PMIDs: {len(scr_zero_scores)} instances\")\n    if len(scr_zero_scores) > 0:\n        print(\"Zero score fields (SCR PMIDs):\")\n        for (field, model), _ in scr_zero_scores.items():\n            print(f\"  {model}: {field}\")\nelse:\n    print(\"No SCR PMID data available for summary statistics\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}