# Results Processing Workflow Structure

## Directory Organization

```
results/
├── raw_results/                          # Raw curation results and initial dataframe
│   ├── gpt-4.1/
│   ├── gpt-4.1-mini/
│   ├── gpt-4.1-nano/
│   ├── gpt-5/
│   ├── gpt-5-mini/
│   ├── gpt-5-nano/
│   ├── combined_dataframe_raw.csv        # Generated by results_processing/generate_all_models_dataframe.py
│   └── combined_dataframe_raw_metadata.json
│
├── cleaned_results/                      # Normalized/harmonized dataframe and reconstructed JSONs
│   ├── combined_dataframe_cleaned.csv    # You'll create this after normalization
│   └── [reconstructed JSONs will go here after running reconstruct_from_combined.py]
│
└── curation_results_summary.md          # Overall summary document
```

## Workflow Steps

### 1. Generate Raw Combined Dataframe
```bash
cd results_processing/
poetry run python generate_all_models_dataframe.py --config config_generate_all_models.json
```
- **Input**: `results/raw_results/[model_directories]` + `ground_truth_fixed/`
- **Output**: `results/raw_results/combined_dataframe_raw.csv`

### 2. Normalize/Harmonize Data (Your Step)
- **Input**: `results/raw_results/combined_dataframe_raw.csv`
- **Process**: Standardize field values across all models
- **Output**: `results/cleaned_results/combined_dataframe_cleaned.csv`

### 3. Reconstruct Clean JSONs
```bash
cd results_processing/
poetry run python reconstruct_from_combined.py --config config_reconstruct_cleaned.json
```
- **Input**: `results/cleaned_results/combined_dataframe_cleaned.csv`
- **Output**: `results/cleaned_results/ground_truth/` + `results/cleaned_results/model_output/[model_name]/`

## Configuration Files

- `results_processing/config_generate_all_models.json` - For generating raw dataframe
- `results_processing/config_reconstruct_cleaned.json` - For reconstructing from cleaned dataframe
- `results_processing/config_reconstruct_combined.json` - For reconstructing from raw dataframe (if needed)

## Key Benefits

- ✅ **Clear separation** of raw vs cleaned results
- ✅ **Preserves original data** in raw_results
- ✅ **Organized workflow** for normalization and comparison
- ✅ **Minimal dataframe** with only experimental metadata (67 columns vs 186)
- ✅ **Ready for scoring** and model evaluation

## Dataframe Structure

The combined dataframe contains:

### Essential Columns (9):
- `data_source`, `model_name`, `hpscreg_name`, `hpscreg_base`, `publication_pmid`
- `json_filename`, `json_filepath`, `_original_json`, `_array_index`

### Experimental Metadata Sections (58 columns):
- **basic_data** (4): cell_line_alt_name, cell_type, frozen, hpscreg_name
- **contact** (6): group, first_name, last_name, name_initials, e_mail, phone_number
- **generator** (1): group
- **publications** (7): doi, journal, title, first_author, last_author, year, pmid
- **donor** (4): age, sex, disease_description, disease_name
- **genomic_modifications** (6): mutation_type, cytoband, delivery_method, description, genotype, loci_name
- **differentiation_results** (5): cell_type, show_potency, marker_list, method_used, description
- **undifferentiated_characterisation** (3): epi_pluri_score, pluri_test_score, pluri_novelty_score
- **genomic_characterisation** (4): passage_number, karyotype, karyotype_method, summary
- **induced_derivation** (7): i_source_cell_type_id, i_source_cell_origin_id, derivation_year, non_int_vector, non_int_vector_name, i_source_cell_type_term, i_source_cell_origin_term
- **embryonic_derivation** (5): embryo_stage, zp_removal_technique, trophectoderm_morphology, icm_morphology, e_preimplant_genetic_diagnosis
- **ethics** (3): ethics_number, approval_date, institutional_HREC
- **culture_medium** (3): co2_concentration, o2_concentration, passage_method

**Total: 67 columns** (119 columns removed from original 186)